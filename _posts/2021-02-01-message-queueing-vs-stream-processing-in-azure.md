---
title: Message Queueing vs. Event Stream Processing in Azure
permalink: /2021/02/01/message-queueing-vs-stream-processing-in-azure
---
![teaser]({{ site.url }}/images/az-messaging/teaser1.png)
# Message Queueing vs.Event Stream Processing in Azure.



Yet another customer got confused and went in wrong direction with various Messaging products (more specifically, with Event Hubs) in Azure, so I finally forced myself to write down some of the typical arguments I hear from them and how I typically try to prove those arguments wrong. My first intention was to call this post something like "Fallacies of Messaging in Azure", but then I remembered that I'm not yet [L. Peter Deutsch](https://en.wikipedia.org/wiki/L._Peter_Deutsch). So let's just call them *false assumptions* and see what they can lead to in practice.

NOTE1: No, I'm not trying to say that e.g. Service Bus is anyhow "better" than e.g. Event Hubs. I'm just highlighting important architectural differences, to once again underscore the importance of picking the right tool for the job and avoiding any sort of [Cargo cult](https://en.wikipedia.org/wiki/Cargo_cult) when making decisions. Any below statements regarding Azure Service Bus apply more or less equally to any [Message Queueing Service](https://en.wikipedia.org/wiki/Message_queuing_service), while any mentioned specificities of Azure Event Hubs are more or less relevant to any [Stream Processing](https://en.wikipedia.org/wiki/Stream_processing) system. For an even more generic overview of these two similar yet different concepts I greatly recommend [this session](https://www.youtube.com/watch?v=avi-TZI9t2I) from Dr. Martin Kleppmann.

NOTE2: All the below demo usecases are illustrated with sample Azure Functions, that come from [this GitHub repo](https://github.com/scale-tone/az-messaging-demo) and are written in TypeScript. Those nice blue 'Deploy to Azure' buttons create Function App instances along with other relevant resources (Event Hubs and Service Bus namespaces, Application Insights instances and Storage Accounts) in your Azure Subscription and deploy source code straight into there with [Kudu App Service](https://docs.microsoft.com/en-us/azure/app-service/deploy-continuous-deployment#option-1-kudu-app-service). Don't forget to remove those resources once done.



## False assumption 1. "Our service is as huge as Office365 (oh, yes, indeed!), so we need more performance, more scalability, more processing rate - more of everything! So we choose Event Hubs."

OK, so [here is a simple sample Azure Function App](https://github.com/scale-tone/az-messaging-demo/tree/main/az-messaging-demo-1-scaling), with two functionally equal event handlers - [EventHubHandler](https://github.com/scale-tone/az-messaging-demo/blob/main/az-messaging-demo-1-scaling/EventHubHandler/index.ts) and [ServiceBusHandler](https://github.com/scale-tone/az-messaging-demo/blob/main/az-messaging-demo-1-scaling/ServiceBusHandler/index.ts). Both do the same thing - take an event, emulate some processing activity by sleeping for 100 ms and then declare the event processed. Also the app contains [this Timer-triggered function](https://github.com/scale-tone/az-messaging-demo/blob/main/az-messaging-demo-1-scaling/TimerTriggeredFunc/index.ts), which produces a constant flow of events towards both of those handlers. You can quickly deploy the whole test setup with this button:

[![Deploy to Azure](https://aka.ms/deploytoazurebutton)](https://portal.azure.com/#create/Microsoft.Template/uri/https%3A%2F%2Fraw.githubusercontent.com%2Fscale-tone%2Faz-messaging-demo%2Fmain%2Faz-messaging-demo-1-scaling%2Farm-template.json)

It will create all relevant resources, including a Function App instance with Dedicated Plan, a Service Bus queue, an Event Hub *with one [partition](https://docs.microsoft.com/en-us/azure/event-hubs/event-hubs-features#partitions) in it* and also a separate Application Insights instance. If you then go to that instance and monitor `EventHubEventProcessed` and `ServiceBusEventProcessed` custom metrics, it will show you a picture similar to this:

![sb-vs-eh-processing-speed]({{ site.url }}/images/az-messaging/sb-vs-eh-processing-speed-1.png)

As you can see, Service Bus event processing (~60 events/sec) goes much faster than Event Hub event processing (~10 events/sec) right from the start, even with one single computing instance (that's because even a single Functions Host efficiently parallelizes Service Bus event processing). If you then go to that Function App's 'Scale Out (App Service Plan)' tab and increase instance count to e.g. 3:

![scale-out]({{ site.url }}/images/az-messaging/scale-out.png)

, Service Bus processing speed will increase even more, while Event Hub processing will remain same as slow.

NOTE: The above 'Deploy to Azure' button is intentionally instructed to deploy the sample into a Premium Plan limited to 1 processing instance, so that you can then manually scale it out. Normally the platform will scale out your Functions instance automatically, but it will not change the picture.

The reason for this behavior is that, unlike a Service Bus queue, an Event Hub partition is a *data stream*. You can think of it as of a magnetic tape in those early days of computing era - it is technically possible to fast-forward or rewind it, but there is no way to increase reading speed by trying to read it 'in parallel'. With Service Bus queues you instantly speed up processing by simply adding more computing instances. With Event Hubs you can achieve something similar by increasing the number of *partitions*, but:
1. That number is [limited](https://docs.microsoft.com/en-us/azure/event-hubs/event-hubs-quotas) to 32 per hub for Standard Tier and 1024 per hub for Dedicated Tier. 
2. *Dynamic* adjustment of this number is [only possible with Dedicated tier](https://docs.microsoft.com/en-us/azure/event-hubs/dynamically-add-partitions). And even in that case it is not as easy as moving a slider - might as well require reconfiguring/restarting your clients (those which are *sending* events). Plus, this change won't affect those events that are *already there*. So if your processing side gets eventually flooded due to an uncareful planning - there's still not much you can do.

Lesson learned: **Azure Event Hubs** is a tool for *event stream* processing, not just event processing (mind the difference). It can *injest* your events in enormous amounts, but keeping up with the pace when *processing* them is still solely your responsibility. If your injestion rate is really so huge that no [Message Queueing Service](https://en.wikipedia.org/wiki/Message_queuing_service) can natively handle it, yet still each and every single event matters - consider hybrid scenarios like **Event Hubs** (for injestion) + **Service Bus** (for further processing), or **Event Hubs** (for injestion) + **Stream Analytics** (for aggregation) + **Service Bus** (for further processing), or **Event Hubs** (for injestion) + **Event Hubs Capture** (for capturing into storage files) + whatever technology you prefer for asynchronously processing those captured files.



## False assumption 2. "We have those tiny little pieces of data that travel across our system. We call them *events*. So we choose Event Hubs, because it has that word in its name."

"Event-driven" is not as simple as you might think. There're many important architectural concepts that accompany this pattern. For example, in your real application you would typically want your event handlers to *dynamically* subscribe and unsubscribe to/from your event source (so that they don't have to dig through the whole history of events first). In Service Bus this is considered first-class citizen functionality and is implemented with [Topics and Subscriptions](https://docs.microsoft.com/en-us/azure/service-bus-messaging/service-bus-queues-topics-subscriptions#topics-and-subscriptions), while with Event Hubs it is much harder to achieve - you would need to configure your event processor host with [EventPosition.FromEnd](https://docs.microsoft.com/en-us/dotnet/api/microsoft.azure.eventhubs.eventposition.fromend?view=azure-dotnet) setting, plus reset the [Offset](https://docs.microsoft.com/en-us/dotnet/api/microsoft.azure.eventhubs.processor.lease.offset?view=azure-dotnet#Microsoft_Azure_EventHubs_Processor_Lease_Offset) value (with Azure Functions that would mean [manually dropping the checkpoint blob from underlying Storage](https://github.com/Azure/azure-functions-eventhubs-extension/issues/64#issuecomment-745728721)).

Another important concept is known as *polymorphic event processing* (or *event routing*), which in practice means, that some of your event handlers might only be interested in some specific *type* of event and not in all events produced by some source. And again, with Service Bus you get this feature out-of-the-box with [Topic Filters](https://docs.microsoft.com/en-us/azure/service-bus-messaging/topic-filters), while Event Hubs do not provide anything like that, so the only option for your handler would be to read the entire stream and filter out relevant events on the client side. Here is another demo setup, that demonstrates this in practice:

[![Deploy to Azure](https://aka.ms/deploytoazurebutton)](https://portal.azure.com/#create/Microsoft.Template/uri/https%3A%2F%2Fraw.githubusercontent.com%2Fscale-tone%2Faz-messaging-demo%2Fmain%2Faz-messaging-demo-2-event-filtering%2Farm-template.json) 

That button will deploy [this demo Function App](https://github.com/scale-tone/az-messaging-demo/tree/main/az-messaging-demo-2-event-filtering), which [streams events](https://github.com/scale-tone/az-messaging-demo/blob/main/az-messaging-demo-2-event-filtering/TimerTriggeredFunc/index.ts) to both an Event Hub and a Service Bus Topic at the rate of **1K/sec** and also handles them with its handlers. Every **1/150-th** event (**~0.7%**) is marked as "green", others are set to be "red", and the handlers are only processing "green" ones. [ServiceBusHandler](https://github.com/scale-tone/az-messaging-demo/blob/main/az-messaging-demo-2-event-filtering/ServiceBusHandler/index.ts) gets "green" events only, because a [correlation filter is applied](https://github.com/scale-tone/az-messaging-demo/blob/main/az-messaging-demo-2-event-filtering/arm-template.json#L107) to its subscription, which reduces the event rate to ridiculous **~7** events/sec, and the handler has no problem with consuming it. [EventHubHandler](https://github.com/scale-tone/az-messaging-demo/blob/main/az-messaging-demo-2-event-filtering/EventHubHandler/index.ts) doesn't have that luxury and is therefore forced to pump the whole stream. Both handlers output custom metrics (`ServiceBusGreenEventAgeInSec` and `EventHubGreenEventAgeInSec`, respectively) representing event's TTL (time difference between event creation and event processing) in seconds, and when rendered in Application Insights, those metrics would look like this:

![sb-vs-eh-event-filtering]({{ site.url }}/images/az-messaging/sb-vs-eh-event-filtering-1.png)

Unlike [ServiceBusHandler](https://github.com/scale-tone/az-messaging-demo/blob/main/az-messaging-demo-2-event-filtering/ServiceBusHandler/index.ts), [EventHubHandler](https://github.com/scale-tone/az-messaging-demo/blob/main/az-messaging-demo-2-event-filtering/EventHubHandler/index.ts) has to deal with the entire overall rate of **1K** events/sec and doesn't seem to be able to cope with it, so the event's TTL slowly grows (e.g. on the picture above it is already ~2 minutes behind).

To resolve this particular bottleneck in this particular scenario you could [increase the batch size](https://github.com/scale-tone/az-messaging-demo/blob/main/az-messaging-demo-2-event-filtering/host.json#L6) for your Event Hubs Trigger, so that your Functions instance can pump the stream faster. Remember though, that this streaming speed is still [limited to 4096 events/sec](https://docs.microsoft.com/en-us/azure/event-hubs/event-hubs-scalability#throughput-units) per each **Throughput Unit**, and that the amount of **Throughput Units** is [limited to 20](https://docs.microsoft.com/en-us/azure/event-hubs/event-hubs-quotas#basic-vs-standard-tiers) per namespace (unless you go for [Dedicated Tier](https://docs.microsoft.com/en-us/azure/event-hubs/event-hubs-faq#dedicated-clusters)). A more generic workaround for Event Hubs would be to implement custom event routing/filtering with e.g. [Azure Stream Analytics](https://docs.microsoft.com/en-us/azure/stream-analytics/event-hubs-output) or [Azure Functions](https://docs.microsoft.com/en-us/azure/azure-functions/functions-bindings-event-hubs).

Lesson learned: never follow the White Paper, aka never make architectural decisions based solely on naming/nomenclature/taxonomy. Try to anticipate as many functional requirements as possible and match those with specific features of chosen instrument/platform. And of course, don't forget to *try it with your own hands* first.



## False assumption 3. "Not only Event Hubs are *bigger* (whatever it means), but they also still guarantee event delivery (aka *reliable*). So we choose Event Hubs."

Consider the following simple math. You have a constant ingestion rate of e.g. 100 events/sec and a processing pipeline built with Event Hubs, which is perfect in any way except that your event handler *intermittently fails and needs to be retried in ~0.5% of cases* (that's in fact a pretty good reliability number of **99.5%**). Under this circumstances, what the resulting *processing rate* could be?

Before saying that it should be exactly 100 events/sec no matter what, remember that Event Hub messages are expected to be processed *in batches* of some significant size (otherwise the whole purpose is defeated). And the only way to retry a single message in a batch is to retry (or more precisely, to skip checkpointing) *the entire batch*. So if we set this batch size to e.g. **256**, then the probability for a batch to be successfully processed will be as low as (0.995 ^ 256) * 100% = **~28%**. Which means, that roughly 2/3 of batches will need to be reprocessed, which increases the *required* processing rate (aka how fast your processing pipeline needs to be to keep up with the pace) to **~172** events/sec. Still not too much, right? But let's see how it all behaves in reality:

[![Deploy to Azure](https://aka.ms/deploytoazurebutton)](https://portal.azure.com/#create/Microsoft.Template/uri/https%3A%2F%2Fraw.githubusercontent.com%2Fscale-tone%2Faz-messaging-demo%2Fmain%2Faz-messaging-demo-3-reliability%2Farm-template.json) 

[This yet another demo Function App](https://github.com/scale-tone/az-messaging-demo/tree/main/az-messaging-demo-3-reliability) produces a constant rate of 100 events/sec and handles them with [ServiceBusHandler](https://github.com/scale-tone/az-messaging-demo/blob/main/az-messaging-demo-3-reliability/ServiceBusHandler/index.ts) and [EventHubHandler](https://github.com/scale-tone/az-messaging-demo/blob/main/az-messaging-demo-3-reliability/EventHubHandler/index.ts). But now both handlers throw an exception in **0.5%** of cases. A Service Bus event will in that case be [automatically abandoned](https://docs.microsoft.com/en-us/azure/azure-functions/functions-bindings-service-bus-trigger?tabs=csharp#peeklock-behavior) and retried later. For Event Hub batches the default Azure Functions behaviour is to [checkpoint every batch no matter whether your handler succeeds or throws](https://docs.microsoft.com/en-us/azure/azure-functions/functions-bindings-event-hubs-trigger?tabs=javascript#scaling), so to make our processing reliable we need to enable [Azure Functions Retry Policies](https://docs.microsoft.com/en-us/azure/azure-functions/functions-bindings-error-pages?tabs=javascript#retry-policies-preview) by doing [some extra configuration](https://github.com/scale-tone/az-messaging-demo/blob/main/az-messaging-demo-3-reliability/EventHubHandler/function.json#L12). Both handlers also output custom metrics (`ServiceBusTimeSinceStartup` and `EventHubTimeSinceStartup`) that show the time difference in seconds between app startup and the current event being processed. If you then render these metrics in the relevant Application Insights instance, the picture will be like this:

![sb-vs-eh-reliability]({{ site.url }}/images/az-messaging/sb-vs-eh-reliability.png)

Which tells us, that [ServiceBusHandler](https://github.com/scale-tone/az-messaging-demo/blob/main/az-messaging-demo-3-reliability/ServiceBusHandler/index.ts) processes its events perfectly on time, while [EventHubHandler](https://github.com/scale-tone/az-messaging-demo/blob/main/az-messaging-demo-3-reliability/EventHubHandler/index.ts) has already started *lagging behind*. This happens because of increased amount of events to be (re)processed, but mostly because of substantial overhead of batches to be retried. If we now remember that Event Hub events have a [relatively small retention period](https://docs.microsoft.com/en-us/azure/event-hubs/event-hubs-quotas#basic-vs-standard-tiers) and will expire (disappear) after that, then the conclusion will be more or less obvious: a solution like the above doesn't look too much reliable.

The workaround in this particular case could of course be to [decrease the batch size](https://github.com/scale-tone/az-messaging-demo/blob/main/az-messaging-demo-3-reliability/EventHubHandler/function.json#L12), so that it doesn't get reverted that often. Or to properly handle all potential exceptions, which in practice might require evolving your own custom retry and dead-lettering logic, to ensure that each and every event is successfully processed.

Lesson learned: it is very important to understand the difference between Delivery Guarantee and Processing Guarantee, and be precisely sure about which type of guarantee you would like to see in your solution. Historically Event Streaming platforms like Azure Event Hubs were intended for... well, massive event streaming and stream analytics, in which case processing of a single particular event is supposed to be done *on a best-effort basis*. Later on they did matured with features to ensure reliable delivery, but those features might still be not enough for you to achieve reliable processing. Plus, don't forget about Event Hubs [retention period](https://docs.microsoft.com/en-us/azure/event-hubs/event-hubs-quotas#basic-vs-standard-tiers), which is never infinite, so if your processing pipeline is for whatever reason not fast enough, eventually it will simply start *losing* events - and that undermines the whole idea of reliability, doesn't it?

Questions, concerns or objections? Let's discuss them [via Twitter](https://twitter.com/tino_scale_tone/status/1356321708425310214)!